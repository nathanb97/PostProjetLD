{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import que nous avons besoin:\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "from threading import Thread\n",
    "import time\n",
    "import dist_levenstein\n",
    "import json\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('max_colwidth',800)\n",
    "\n",
    "\n",
    "\n",
    "# fonction general:\n",
    "## fonction d'insertion dictionnaire\n",
    "def insert_word(dic,word):\n",
    "    word=word.lower()\n",
    "    word=word.replace(\".\",\"\")\n",
    "    word=word.replace(\"!\",\"\")\n",
    "    word=word.replace(\"\\n\",\"\")\n",
    "    word=word.replace(\"#\",\"\")\n",
    "    word=word.replace(\"()\",\"\")\n",
    "    word=word.replace(\")\",\"\")\n",
    "    word=word.replace(\",\",\"\")\n",
    "    word=word.replace(\"\\\"\",\"\")\n",
    "    if word not in dic:\n",
    "        dic[word]=1\n",
    "    else:\n",
    "        dic[word]+=1\n",
    "## fonction de recomendation de mot en cas de faute d'ortographe \n",
    "def recommended_word(word):\n",
    "    # On reduit le champ de recherche ou sinon trop long \n",
    "    tmp=datframe_word.loc[(datframe_word[\"word\"].str.len()<=len(word)+1) & (datframe_word[\"word\"].str.len()>=len(word)-1)]\n",
    "    tmp[\"dist_levenstein\"]=tmp[\"word\"].apply(lambda x : dist_levenstein.DistanceDeLevenshtein(x.lower(),word.lower()))\n",
    "    return list(tmp.loc[tmp[\"dist_levenstein\"]<3].sort_values(\"dist_levenstein\",ascending=1)[\"word\"])    \n",
    "## fonction de recomendation de pays en cas de faute d'ortographe\n",
    "def recommended_country(country):\n",
    "    tmp=country_for_leven.copy()\n",
    "    tmp[\"dist_levenstein\"]=tmp[\"country\"].apply(lambda x : dist_levenstein.DistanceDeLevenshtein(x.lower(),country.lower()))\n",
    "    return list(tmp.loc[tmp[\"dist_levenstein\"]<3].sort_values(\"dist_levenstein\",ascending=1)[\"country\"])   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Quelques variable globale\n",
    "tweet = pd.read_csv('./tweets.csv', sep = ',') #Mon dataframe tweet\n",
    "tweet[\"word\"]=np.nan\n",
    "\n",
    "liste_date=list(tweet[\"date\"])\n",
    "date_def=liste_date[int(len(liste_date)/2)]\n",
    "date_def=date_def[:11]+re.sub(r'[0-9]',\"0\",date_def[11:]) #Une date al√©atoire\n",
    "                    \n",
    "                    \n",
    "#Un dataFrame de tous mes mots contenue dans tweet\n",
    "dict_word={}\n",
    "for i in tweet[\"text\"]:\n",
    "    for word in i.split(\" \"):\n",
    "        insert_word(dict_word,word)\n",
    "datframe_word=pd.DataFrame.from_dict(dict_word,orient=\"index\",columns=[\"count_word\"]).sort_values(\"count_word\",ascending=0)\n",
    "datframe_word['word']=datframe_word.index\n",
    "datframe_word['dist_levenstein']=np.nan\n",
    "datframe_word.index=[i for i in range(0,len(datframe_word))]\n",
    "#Un dataFrame de tous mes pays contenue dans tweet\n",
    "country_for_leven=pd.DataFrame((tweet[\"place_country\"].unique()),columns=[\"country\"]).dropna()\n",
    "datframe_word['dist_levenstein']=np.nan\n",
    "dict_word={}\n",
    "word=\"hahpy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction for Javascript pour faire des requetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_by_name(name):\n",
    "    return tweet[tweet[\"user_name\"]==name].to_json()\n",
    "def tweet_by_place_name(name):\n",
    "    return tweet[tweet[\"place_name\"]==name].to_json()\n",
    "def tweet_by_id(name):\n",
    "    return tweet[tweet[\"id\"]==name].to_json()\n",
    "def tweet_by_user_id(name):\n",
    "    return tweet[tweet[\"user_id\"]==name].to_json()\n",
    "def search_by_word_only(res,word):\n",
    "    res=res[res[\"text\"].str.contains(r\"\\b\"+word+r\"\\b\",case=False)]\n",
    "    tmp=tweet[tweet[\"text\"].str.contains(r\"\\b\"+word+r\"\\b\",case=False)]\n",
    "                                         \n",
    "    if(len(tmp)==0):\n",
    "        liste=recommended_word(word)\n",
    "        #print(\"Aucun mot vous voulez dire plutot\",liste)\n",
    "    else:\n",
    "        liste=[]\n",
    "    if(len(res)==0):\n",
    "        print(\"Aucun mot est present pour\",word)\n",
    "    return res,liste\n",
    "def search_by_word_or(word):\n",
    "    tmp=tweet[tweet[\"text\"].str.contains(r\"\\b\"+word+r\"\\b\",case=False)]\n",
    "    tmp[\"word\"]=word\n",
    "    if(len(tmp)==0):\n",
    "        liste=recommended_word(word)\n",
    "        #print(\"Aucun mot vous voulez dire plutot\",liste)\n",
    "    else:\n",
    "        liste=[]\n",
    "    print(word,len(tmp))\n",
    "    return tmp,liste\n",
    "\n",
    "def search_by_country(res,country):\n",
    "    tmp=res[res[\"place_country\"].str.contains(r\"\\b\"+country+r\"\\b\",case=False,na=False)]\n",
    "    tmp[\"country\"]=country\n",
    "    verif=len(country_for_leven[country_for_leven[\"country\"].str.contains(r\"\\b\"+country+r\"\\b\",case=False)])\n",
    "    if(verif==0):\n",
    "        liste=recommended_country(country)\n",
    "    else:\n",
    "        liste=[]\n",
    "    return tmp,liste\n",
    "#Cherche tweet apr√®s la date de la forme jj/mm/aaaa\n",
    "def search_by_date1(res,date):\n",
    "    #J'attribue la meme forme de date grace a date_def\n",
    "    date_res=date[6:]+\"-\"+date[3:5]+\"-\"+date[:2]+'T'+date_def[11:]\n",
    "    print(date_res)\n",
    "    return res[res[\"date\"]>date_res]\n",
    "#Cherche tweet avant la date de la forme jj/mm/aaaa\n",
    "def search_by_date2(res,date):\n",
    "    #J'attribue la meme forme de date grace a date_def\n",
    "    date_res=date[6:]+\"-\"+date[3:5]+\"-\"+date[:2]+'T'+date_def[11:]\n",
    "    print(date_res)\n",
    "    return res[res[\"date\"]>date_res]\n",
    "\n",
    "def get_union_hastag(res):\n",
    "    liste=[]\n",
    "    #print(len(res))\n",
    "    for i in range(0,3):\n",
    "        union_hastag=res.copy()\n",
    "        union_hastag[res.columns]\n",
    "        union_hastag=union_hastag.drop(columns=[\"hashtag_\"+str((i+1)%3),\"hashtag_\"+str((i+2)%3)])\n",
    "        union_hastag=union_hastag.rename(columns={\"hashtag_\"+str(i):\"hashtag\"})\n",
    "        #print(len(union_hastag))\n",
    "        #union_hastag=union_hastag.dropna(subset=[\"hashtag\"])\n",
    "        #print(len(union_hastag))\n",
    "        liste.append(union_hastag)\n",
    "    return pd.concat(liste,ignore_index=True)\n",
    "\n",
    "def get_union_hahstag(res):\n",
    "    liste=[]\n",
    "    #print(len(res))\n",
    "    for i in range(0,3):\n",
    "        union_hastag=res.copy()\n",
    "        union_hastag[res.columns]\n",
    "        union_hastag=union_hastag.drop(columns=[\"hashtag_\"+str((i+1)%3),\"hashtag_\"+str((i+2)%3)])\n",
    "        union_hastag=union_hastag.rename(columns={\"hashtag_\"+str(i):\"hashtag\"})\n",
    "        #print(len(union_hastag))\n",
    "        #union_hastag=union_hastag.dropna(subset=[\"hashtag\"])\n",
    "        #print(len(union_hastag))\n",
    "        liste.append(union_hastag)\n",
    "    return pd.concat(liste,ignore_index=True).reset_index()\n",
    "def search_by_serveral_word_or(words):\n",
    "    listes_mot=words.split(\"_\")\n",
    "    dic_corrected=dict()\n",
    "    find=False\n",
    "    res=tweet.copy()\n",
    "    cpt=0\n",
    "    for i in range(0,len(listes_mot)):\n",
    "        word=listes_mot[i]\n",
    "        if(len(word)==0):\n",
    "            continue\n",
    "        tmp,l=search_by_word_or(word)\n",
    "        if len(l)>0:\n",
    "            dic_corrected[word]=l\n",
    "        elif len(tmp)==0:\n",
    "            True\n",
    "        else:\n",
    "            if(cpt>0):\n",
    "                res=pd.concat([res, tmp], ignore_index=True)\n",
    "                res.drop_duplicates(subset=[\"id\"])\n",
    "            else:\n",
    "                res=tmp\n",
    "                cpt+=1\n",
    "            find=True\n",
    "    for i in dic_corrected:\n",
    "        print(\"Aucun mot ne contient\",i,\"vous voulez dire pour ce mot\",dic_corrected[i])\n",
    "    return res,dic_corrected\n",
    "\n",
    "def search_by_serveral_country(base,countrys):\n",
    "    listes_mot=countrys.split(\"_\")\n",
    "    dic_corrected=dict()\n",
    "    find=False\n",
    "    res=base.copy()\n",
    "    cpt=0\n",
    "    for i in range(0,len(listes_mot)):\n",
    "        country=listes_mot[i]\n",
    "        if(len(country)==0):\n",
    "            continue\n",
    "        tmp,l=search_by_country(base,country)\n",
    "        if len(l)>0:\n",
    "            dic_corrected[country]=l\n",
    "        elif len(tmp)==0:\n",
    "            True\n",
    "        else:\n",
    "            if(cpt>0):\n",
    "                res=pd.concat([res, tmp], ignore_index=True)\n",
    "                res.drop_duplicates(subset=[\"id\"])\n",
    "            else:\n",
    "                res=tmp\n",
    "                cpt+=1\n",
    "            find=True\n",
    "    for i in dic_corrected:\n",
    "        print(\"Aucun mot ne contient\",i,\"vous voulez dire pour ce mot\",dic_corrected[i])\n",
    "    return res,dic_corrected\n",
    "\n",
    "def request_tweet(words=\"\",name_country=\"\",date_start=\"\",date_end=\"\"):\n",
    "    #Requetes mot:\n",
    "    res,l=search_by_serveral_word_or(words)\n",
    "    words=words.split(\"_\")\n",
    "    countrys=name_country.split(\"_\")\n",
    "\n",
    "    #Requetes pays\n",
    "    reco_country={}\n",
    "    if name_country!=\"\":\n",
    "        res,reco_country=search_by_serveral_country(res,name_country)\n",
    "    #Requetes dates\n",
    "    if date_start!=\"\":\n",
    "        res=search_by_date1(res,date_start)\n",
    "    if date_end!=\"\":\n",
    "        res=search_by_date2(res,date_end)\n",
    "    #Avoir le plus grand nombre de tweet par  pays :\n",
    "    pays=res.groupby(\"place_country\").count().sort_values(\"id\",ascending=0)\n",
    "    pays=pays[['id']].rename(columns={\"id\":\"total_tweets\"}).reset_index()\n",
    "    pays[\"count_tweet_per_word\"]=np.nan\n",
    "    for country in pays[\"place_country\"].unique():\n",
    "        count_tweet_per_word=pd.DataFrame()\n",
    "        for word in words:\n",
    "            if(len(word)==0):\n",
    "                continue\n",
    "            count_tweet_per_word=count_tweet_per_word.append({\"word\":word,\"count_tweet\":len(res.loc[(res[\"place_country\"]==country)&(res[\"word\"]==word)])},ignore_index=True)\n",
    "            #count_tweet_per_word.append({\"word\":word,\"count_tweet\":len(res[res[\"word\"]]==word)},ignore_index=True)\n",
    "        pays.loc[pays[\"place_country\"]==country,\"count_tweet_per_word\"]=pays.loc[pays[\"place_country\"]==country,\"count_tweet_per_word\"].apply(lambda x : count_tweet_per_word.to_dict(orient='records'))\n",
    "\n",
    "    #Avoir le top 10 hashtag:\n",
    "    union_hashtag=get_union_hastag(res).reset_index().groupby('hashtag').count().sort_values(\"index\",ascending=0).iloc[0:10][[\"id\"]].reset_index().rename(columns={\"id\":\"count_hashtag\"})\n",
    "    #Avoir les longueur :\n",
    "    latitude_longitude=pd.DataFrame()\n",
    "    for word in words:\n",
    "        how_co=len(res[res[\"word\"]==word])\n",
    "        if(how_co)==0:\n",
    "            continue\n",
    "\n",
    "        latitude_longitude=latitude_longitude.append({\"word\":word,\"coordinations\":res[res[\"word\"]==word][[\"longitude\",\"latitude\"]].copy().to_dict(orient='records')},ignore_index=True)\n",
    "    latitude_longitude_by_country=pd.DataFrame()\n",
    "    try:\n",
    "        for country in res[\"place_country\"].unique():\n",
    "            print(country)\n",
    "            if(len(country)==0):\n",
    "                countinue\n",
    "            lat_lon=np.mean(res[res[\"place_country\"]==country][[\"longitude\",\"latitude\"]])\n",
    "            how_co=len(res[res[\"place_country\"]==country])\n",
    "            if(how_co)==0:\n",
    "                continue\n",
    "            latitude_longitude_by_country=latitude_longitude_by_country.append({\"country\":country,\"longitude\":lat_lon[\"longitude\"],\n",
    "                                                                                \"latitude\":lat_lon[\"latitude\"],\"count\":how_co},ignore_index=True)\n",
    "    except:\n",
    "        print(\"grossee erreur je peux pas faire la map\")\n",
    "        True\n",
    "\n",
    "    print(l)\n",
    "    count_tweet=pd.DataFrame()\n",
    "    count_tweet=count_tweet.append({\"word_count\":len(words),\"totally_tweet_count\":len(res),\"tweets_per_country\":pays.to_dict(orient=\"records\"),\"top_hashtags\":union_hashtag.to_dict(orient=\"records\"),\"tweets_latitude_longitude\":latitude_longitude.to_dict(orient=\"records\"),\"latitude_longitude_by_country\":latitude_longitude_by_country.to_dict(orient=\"records\"),\"recommended_word\":l,\"recommended_country\":reco_country},ignore_index=True)\n",
    "    return count_tweet.to_dict(orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "France 54\n",
      "France\n",
      "Fran√ßa\n",
      "Francia\n",
      "Brasil\n",
      "Bahrain\n",
      "Italie\n",
      "Êó•Êú¨\n",
      "Royaume-Uni\n",
      "Peru\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brami/Bureau/Projet_language_dynamique/virtualenv/lib/python3.6/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'latitude_longitude_by_country': [{'country': 'France',\n",
       "    'latitude': 47.87854065428572,\n",
       "    'longitude': 2.5551584911904763},\n",
       "   {'country': 'Fran√ßa', 'latitude': 48.8567, 'longitude': 2.3508},\n",
       "   {'country': 'Francia', 'latitude': 47.970285242, 'longitude': 2.386582764},\n",
       "   {'country': 'Brasil', 'latitude': -11.0, 'longitude': -53.0},\n",
       "   {'country': 'Bahrain', 'latitude': 26.23844302, 'longitude': 50.59505533},\n",
       "   {'country': 'Italie', 'latitude': 45.09950585, 'longitude': 7.64490582},\n",
       "   {'country': 'Êó•Êú¨', 'latitude': 34.67740556, 'longitude': 135.20386934},\n",
       "   {'country': 'Royaume-Uni', 'latitude': 55.8534, 'longitude': -4.23321},\n",
       "   {'country': 'Peru', 'latitude': -12.07113351, 'longitude': -77.05148359}],\n",
       "  'recommended_country': {},\n",
       "  'recommended_word': {},\n",
       "  'top_hashtags': [{'hashtag': 'france', 'count_hashtag': 4},\n",
       "   {'hashtag': 'paris', 'count_hashtag': 2},\n",
       "   {'hashtag': 'love', 'count_hashtag': 2},\n",
       "   {'hashtag': 'dgstudio', 'count_hashtag': 2},\n",
       "   {'hashtag': 'ARGENTINA', 'count_hashtag': 1},\n",
       "   {'hashtag': 'montparnasse', 'count_hashtag': 1},\n",
       "   {'hashtag': 'narghile', 'count_hashtag': 1},\n",
       "   {'hashtag': 'natation', 'count_hashtag': 1},\n",
       "   {'hashtag': 'newyear', 'count_hashtag': 1},\n",
       "   {'hashtag': 'newyeartravel', 'count_hashtag': 1}],\n",
       "  'totally_tweet_count': 54.0,\n",
       "  'tweets_latitude_longitude': [{'coordinations': [{'longitude': 2.71340648,\n",
       "      'latitude': 48.67916368},\n",
       "     {'longitude': 2.3522218999999995, 'latitude': 48.856614},\n",
       "     {'longitude': 2.333764, 'latitude': 48.911856},\n",
       "     {'longitude': 5.9324900000000005, 'latitude': 43.12215},\n",
       "     {'longitude': -0.5523239999999999, 'latitude': 47.471343},\n",
       "     {'longitude': -0.5783, 'latitude': 44.8386},\n",
       "     {'longitude': 2.3508, 'latitude': 48.8567},\n",
       "     {'longitude': 5.4245, 'latitude': 43.2905},\n",
       "     {'longitude': 2.3508, 'latitude': 48.8567},\n",
       "     {'longitude': 7.33568496, 'latitude': 48.06742056},\n",
       "     {'longitude': 2.3508, 'latitude': 48.8567},\n",
       "     {'longitude': -1.2249, 'latitude': 49.37871},\n",
       "     {'longitude': 2.3508, 'latitude': 48.8567},\n",
       "     {'longitude': 6.329330000000001, 'latitude': 45.85846},\n",
       "     {'longitude': 2.3508, 'latitude': 48.8567},\n",
       "     {'longitude': 2.3508, 'latitude': 48.8567},\n",
       "     {'longitude': 2.58180305, 'latitude': 50.45161566},\n",
       "     {'longitude': 1.4443, 'latitude': 43.6044},\n",
       "     {'longitude': 3.0684, 'latitude': 50.6281},\n",
       "     {'longitude': 2.3508, 'latitude': 48.8567},\n",
       "     {'longitude': 2.33539467, 'latitude': 48.86138133},\n",
       "     {'longitude': -0.3251299, 'latitude': 45.69379},\n",
       "     {'longitude': 7.7521113, 'latitude': 48.5734053},\n",
       "     {'longitude': -0.5783, 'latitude': 44.8386},\n",
       "     {'longitude': 2.38530189, 'latitude': 48.85448088},\n",
       "     {'longitude': 2.2394303, 'latitude': 48.89085966},\n",
       "     {'longitude': 2.3508, 'latitude': 48.8567},\n",
       "     {'longitude': 2.3508, 'latitude': 48.8567},\n",
       "     {'longitude': 1.8746599999999998, 'latitude': 47.9502487},\n",
       "     {'longitude': 1.41585214, 'latitude': 49.06678023},\n",
       "     {'longitude': 2.3508, 'latitude': 48.8567},\n",
       "     {'longitude': 6.86667, 'latitude': 45.9167},\n",
       "     {'longitude': -2.347, 'latitude': 46.725},\n",
       "     {'longitude': -1.15164, 'latitude': 46.159439899999995},\n",
       "     {'longitude': 1.3, 'latitude': 45.1167},\n",
       "     {'longitude': -53.0, 'latitude': -11.0},\n",
       "     {'longitude': 0.6892, 'latitude': 47.3936},\n",
       "     {'longitude': 2.71340648, 'latitude': 48.67916368},\n",
       "     {'longitude': -4.3, 'latitude': 48.2},\n",
       "     {'longitude': 50.59505533, 'latitude': 26.23844302},\n",
       "     {'longitude': 2.32799536, 'latitude': 48.8757098},\n",
       "     {'longitude': 7.3391649999999995, 'latitude': 47.748090000000005},\n",
       "     {'longitude': 3.0684, 'latitude': 50.6281},\n",
       "     {'longitude': 7.75, 'latitude': 48.5833},\n",
       "     {'longitude': 7.2408, 'latitude': 47.81118},\n",
       "     {'longitude': 2.29455382, 'latitude': 48.85838631},\n",
       "     {'longitude': 7.64490582, 'latitude': 45.09950585},\n",
       "     {'longitude': 135.20386934, 'latitude': 34.67740556},\n",
       "     {'longitude': -4.23321, 'latitude': 55.8534},\n",
       "     {'longitude': 2.3508, 'latitude': 48.8567},\n",
       "     {'longitude': 3.3338330000000003, 'latitude': 49.784275},\n",
       "     {'longitude': 2.0, 'latitude': 47.0},\n",
       "     {'longitude': 2.35649, 'latitude': 48.81501},\n",
       "     {'longitude': -77.05148359, 'latitude': -12.07113351}],\n",
       "    'word': 'France'}],\n",
       "  'tweets_per_country': [{'place_country': 'France',\n",
       "    'total_tweets': 42,\n",
       "    'count_tweet_per_word': [{'count_tweet': 42.0, 'word': 'France'}]},\n",
       "   {'place_country': 'Francia',\n",
       "    'total_tweets': 5,\n",
       "    'count_tweet_per_word': [{'count_tweet': 5.0, 'word': 'France'}]},\n",
       "   {'place_country': 'Bahrain',\n",
       "    'total_tweets': 1,\n",
       "    'count_tweet_per_word': [{'count_tweet': 1.0, 'word': 'France'}]},\n",
       "   {'place_country': 'Brasil',\n",
       "    'total_tweets': 1,\n",
       "    'count_tweet_per_word': [{'count_tweet': 1.0, 'word': 'France'}]},\n",
       "   {'place_country': 'Fran√ßa',\n",
       "    'total_tweets': 1,\n",
       "    'count_tweet_per_word': [{'count_tweet': 1.0, 'word': 'France'}]},\n",
       "   {'place_country': 'Italie',\n",
       "    'total_tweets': 1,\n",
       "    'count_tweet_per_word': [{'count_tweet': 1.0, 'word': 'France'}]},\n",
       "   {'place_country': 'Peru',\n",
       "    'total_tweets': 1,\n",
       "    'count_tweet_per_word': [{'count_tweet': 1.0, 'word': 'France'}]},\n",
       "   {'place_country': 'Royaume-Uni',\n",
       "    'total_tweets': 1,\n",
       "    'count_tweet_per_word': [{'count_tweet': 1.0, 'word': 'France'}]},\n",
       "   {'place_country': 'Êó•Êú¨',\n",
       "    'total_tweets': 1,\n",
       "    'count_tweet_per_word': [{'count_tweet': 1.0, 'word': 'France'}]}],\n",
       "  'word_count': 1.0}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request_tweet(words=\"Mau\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "France\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brami/Bureau/Projet_language_dynamique/virtualenv/lib/python3.6/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'latitude_longitude_by_country': [{'country': 'France',\n",
       "    'latitude': 47.47995707524997,\n",
       "    'longitude': 2.7193864355}],\n",
       "  'recommended_country': {},\n",
       "  'recommended_word': {},\n",
       "  'top_hashtags': [{'hashtag': 'Mar√©e', 'count_hashtag': 8},\n",
       "   {'hashtag': 'perdu', 'count_hashtag': 6},\n",
       "   {'hashtag': 'chien', 'count_hashtag': 5},\n",
       "   {'hashtag': 'paris', 'count_hashtag': 4},\n",
       "   {'hashtag': 'chat', 'count_hashtag': 4},\n",
       "   {'hashtag': 'trouv√©', 'count_hashtag': 3},\n",
       "   {'hashtag': 'T36', 'count_hashtag': 2},\n",
       "   {'hashtag': 'dgstudio', 'count_hashtag': 2},\n",
       "   {'hashtag': 'eiffeltower', 'count_hashtag': 2},\n",
       "   {'hashtag': 'coffee', 'count_hashtag': 2}],\n",
       "  'totally_tweet_count': 160.0,\n",
       "  'tweets_latitude_longitude': [],\n",
       "  'tweets_per_country': [{'place_country': 'France',\n",
       "    'total_tweets': 160,\n",
       "    'count_tweet_per_word': []}],\n",
       "  'word_count': 1.0}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request_tweet(name_country=\"France\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5abc0a99fceb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecommended_country\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"France\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "l,tmp=recommended_country(\"France\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>dist_levenstein</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Indonesia</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mexico</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Êó•Êú¨</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United States</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>Srbija</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>Malta</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>Br√©sil</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>Burkina Faso</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>Chipre</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>312 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           country  dist_levenstein\n",
       "0        Indonesia                8\n",
       "1           Mexico                5\n",
       "2         Thailand                7\n",
       "3               Êó•Êú¨                6\n",
       "4    United States               12\n",
       "..             ...              ...\n",
       "308         Srbija                5\n",
       "309          Malta                5\n",
       "310         Br√©sil                5\n",
       "311   Burkina Faso               10\n",
       "312         Chipre                5\n",
       "\n",
       "[312 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[tmp'']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonctions En attente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_by_name(name):\n",
    "    return tweet[tweet[\"user_name\"]==name]\n",
    "def tweet_by_country(country):\n",
    "    return tweet[tweet[\"place_country\"]==country]\n",
    "def tweet_by_place_name(name):\n",
    "    return tweet[tweet[\"place_name\"]==name]\n",
    "def tweet_by_id(name):\n",
    "    return tweet[tweet[\"id\"]==name]\n",
    "def tweet_by_user_id(name):\n",
    "    return tweet[tweet[\"user_id\"]==name]\n",
    "def search_by_word(res,word):\n",
    "    res=res[res[\"text\"].str.contains(r\"\\b\"+word+r\"\\b\",case=False)]\n",
    "    tmp=tweet[tweet[\"text\"].str.contains(r\"\\b\"+word+r\"\\b\",case=False)]\n",
    "                                         \n",
    "    if(len(tmp)==0):\n",
    "        liste=recommended_word(word)\n",
    "        #print(\"Aucun mot vous voulez dire plutot\",liste)\n",
    "    else:\n",
    "        liste=[]\n",
    "    if(len(res)==0):\n",
    "        print(\"Aucun mot est present pour\",word)\n",
    "    return res,liste\n",
    "\n",
    "def search_by_serveral_word(words):\n",
    "    listes_mot=words.split(\",\")\n",
    "    dic_corrected=dict()\n",
    "    find=False\n",
    "    res=tweet.copy()\n",
    "    for i in range(0,len(listes_mot)):\n",
    "        word=listes_mot[i]\n",
    "        if (i==0 and find==False):\n",
    "            tmp,l=search_by_word(res,word)\n",
    "        else:\n",
    "            tmp,l=search_by_word(res,word)\n",
    "        if len(l)>0:\n",
    "            dic_corrected[word]=l\n",
    "        elif len(tmp)==0:\n",
    "            True\n",
    "        else:\n",
    "            res=tmp\n",
    "            find=True\n",
    "    for i in dic_corrected:\n",
    "        print(\"Aucun mot ne contient\",i,\"vous voulez dire pour ce mot\",dic_corrected[i])\n",
    "    return res,dic_corrected\n",
    "def search_by_word_or(word):\n",
    "    tmp=tweet[tweet[\"text\"].str.contains(r\"\\b\"+word+r\"\\b\",case=False)]\n",
    "    if(len(tmp)==0):\n",
    "        liste=recommended_word(word)\n",
    "        #print(\"Aucun mot vous voulez dire plutot\",liste)\n",
    "    else:\n",
    "        liste=[]\n",
    "    print(word,len(tmp))\n",
    "    return tmp,liste\n",
    "\n",
    "def search_by_serveral_word_or(words):\n",
    "    listes_mot=words.split(\",\")\n",
    "    dic_corrected=dict()\n",
    "    find=False\n",
    "    res=tweet.copy()\n",
    "    cpt=0\n",
    "    for i in range(0,len(listes_mot)):\n",
    "        word=listes_mot[i]\n",
    "        if (i==0 and find==False):\n",
    "            tmp,l=search_by_word_or(word)\n",
    "        else:\n",
    "            tmp,l=search_by_word_or(word)\n",
    "        if len(l)>0:\n",
    "            dic_corrected[word]=l\n",
    "        elif len(tmp)==0:\n",
    "            True\n",
    "        else:\n",
    "            if(cpt>0):\n",
    "                res=pd.concat([res, tmp], ignore_index=True)\n",
    "                res.drop_duplicates(subset=[\"id\"])\n",
    "            else:\n",
    "                res=tmp\n",
    "                cpt+=1\n",
    "            find=True\n",
    "    for i in dic_corrected:\n",
    "        print(\"Aucun mot ne contient\",i,\"vous voulez dire pour ce mot\",dic_corrected[i])\n",
    "    return res,dic_corrected\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy 401\n",
      "yappy 0\n",
      "Mau 11\n",
      "Aucun mot ne contient yappy vous voulez dire pour ce mot ['happy', 'appt', 'üêΩhappy', 'ü•Çhappy', '(happy', 'üéâhappy', 'rappa', 'yatay', 'happ≈∑', 'yayyy', 'yapƒ±≈ü', 'apps', 'yapƒ±', 'puppy', 'yapƒ±m', 'apply', '2happy', 'happpy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brami/Bureau/Projet_language_dynamique-Nathan/ouss/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>user_followers_count</th>\n",
       "      <th>text</th>\n",
       "      <th>place_name</th>\n",
       "      <th>place_country</th>\n",
       "      <th>place_country_code</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>lang</th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>hashtag_0</th>\n",
       "      <th>hashtag_1</th>\n",
       "      <th>hashtag_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1080002955217129472</td>\n",
       "      <td>132400580</td>\n",
       "      <td>Alfonso Bueno</td>\n",
       "      <td>alfonso_b1</td>\n",
       "      <td>134</td>\n",
       "      <td>Feliz a√±o nuevo! Happy new year! #2019 @ Nuevo Laredo, Tamaulipas https://t.co/dVMk1Pl7Et</td>\n",
       "      <td>Nuevo Laredo, Tamaulipas</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>MX</td>\n",
       "      <td>-99.507100</td>\n",
       "      <td>27.485600</td>\n",
       "      <td>es</td>\n",
       "      <td>2019-01-01T07:29:04+00:00</td>\n",
       "      <td>1546327744665</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1080005505337249792</td>\n",
       "      <td>2209026173</td>\n",
       "      <td>aylnsnmz</td>\n",
       "      <td>mcrkz16</td>\n",
       "      <td>74</td>\n",
       "      <td>Happy new year olur in≈üallahüôèüèºüòÖ #konuyubiliyorsunTanrƒ±m(amin) (@ Home Sweet Homeüè°üíö in Toronto, ON) https://t.co/ruozePd27j https://t.co/eM3rOeQHgF</td>\n",
       "      <td>Toronto, Ontario</td>\n",
       "      <td>Canada</td>\n",
       "      <td>CA</td>\n",
       "      <td>-79.397235</td>\n",
       "      <td>43.759598</td>\n",
       "      <td>en</td>\n",
       "      <td>2019-01-01T07:39:12+00:00</td>\n",
       "      <td>1546328352661</td>\n",
       "      <td>konuyubiliyorsunTanrƒ±m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1080005899618537473</td>\n",
       "      <td>35763405</td>\n",
       "      <td>Alex Amado</td>\n",
       "      <td>AlexAmado2002</td>\n",
       "      <td>547</td>\n",
       "      <td>Wishing everyone a Happy New 2019! @ At Cheonggyecheon Stream https://t.co/8imInI7vaN</td>\n",
       "      <td>Jongno-gu, Republic of Korea</td>\n",
       "      <td>Republic of Korea</td>\n",
       "      <td>KR</td>\n",
       "      <td>126.978992</td>\n",
       "      <td>37.569142</td>\n",
       "      <td>en</td>\n",
       "      <td>2019-01-01T07:40:46+00:00</td>\n",
       "      <td>1546328446665</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1080006864295874560</td>\n",
       "      <td>702750804894052353</td>\n",
       "      <td>Ananth Vikram</td>\n",
       "      <td>AnanthVikram85</td>\n",
       "      <td>67</td>\n",
       "      <td>Wishing All a very Happy and Artistic New Year 2019 ahead!! üéÇüç´üôèüòá \\nIf around Bangalore South, kindly make over for a solo recital by my Baby Mahati Ananth @ National College Grounds,‚Ä¶ https://t.co/MzdwCGo4ri</td>\n",
       "      <td>Bengaluru South, India</td>\n",
       "      <td>India</td>\n",
       "      <td>IN</td>\n",
       "      <td>77.544640</td>\n",
       "      <td>12.939070</td>\n",
       "      <td>en</td>\n",
       "      <td>2019-01-01T07:44:36+00:00</td>\n",
       "      <td>1546328676662</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1080007355020890113</td>\n",
       "      <td>16356124</td>\n",
       "      <td>markcoccio</td>\n",
       "      <td>markcoccio</td>\n",
       "      <td>1339</td>\n",
       "      <td>Happy 2019 Everyone.....man oh man the goals I have this year are HUGE and scary, but stand by for updates and check-ins along the way, it is going to be fun. @ Burbank, California https://t.co/g553Myz1Su</td>\n",
       "      <td>Burbank, CA</td>\n",
       "      <td>United States</td>\n",
       "      <td>US</td>\n",
       "      <td>-118.342900</td>\n",
       "      <td>34.169200</td>\n",
       "      <td>en</td>\n",
       "      <td>2019-01-01T07:46:33+00:00</td>\n",
       "      <td>1546328793660</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>1080878759417430016</td>\n",
       "      <td>92357502</td>\n",
       "      <td>Afra Augesti</td>\n",
       "      <td>avraaugesty</td>\n",
       "      <td>691</td>\n",
       "      <td>Malam ini ceritanya lagi mau nyoba ngedit video di laptop. Tetiba nemu 3 rekaman \"dibuang sayang\" ini.\\n.\\nEto-etonya, di video itu, kami lagi diwawancara penyiar Capital FM, salah satu‚Ä¶ https://t.co/5FvQJGT8iA</td>\n",
       "      <td>Dhaka, Bangladesh</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>BD</td>\n",
       "      <td>90.433209</td>\n",
       "      <td>23.813322</td>\n",
       "      <td>in</td>\n",
       "      <td>2019-01-03T17:29:12+00:00</td>\n",
       "      <td>1546536552658</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>1083179888805380097</td>\n",
       "      <td>773709250182934532</td>\n",
       "      <td>jogjajourney</td>\n",
       "      <td>jogjajourney1</td>\n",
       "      <td>33</td>\n",
       "      <td>Mantai yuuk...\\n\\nüì∑ by laila_journey \\n\\nMau?\\nReservasi sekarang juga !\\nCall/text/WA/TG : +6287705576633\\n\\nMore information :\\naristiana_jogjajourney lopinda_journey @utamipipit nisrinanazih‚Ä¶ https://t.co/jT6HS63Yi7</td>\n",
       "      <td>Danurejan, Indonesia</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>ID</td>\n",
       "      <td>110.368000</td>\n",
       "      <td>-7.795850</td>\n",
       "      <td>in</td>\n",
       "      <td>2019-01-10T01:53:04+00:00</td>\n",
       "      <td>1547085184657</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>1083401499076640773</td>\n",
       "      <td>2726285827</td>\n",
       "      <td>Cinere Scooter Club</td>\n",
       "      <td>VespaCinere</td>\n",
       "      <td>1782</td>\n",
       "      <td>#Repost from scooter_beang_kerok with https://t.co/bdeEaPnh03 \\n ... \\n\\nReposted from pesi_official -  Buat sedulur se vespa mari budayakan kembali 1 VESPA BERJUTA SODARA.. jadi...bagi sedulur yg mau‚Ä¶ https://t.co/U2ozy7mmrV</td>\n",
       "      <td>Lima, Indonesia</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>ID</td>\n",
       "      <td>106.782747</td>\n",
       "      <td>-6.334487</td>\n",
       "      <td>in</td>\n",
       "      <td>2019-01-10T16:33:40+00:00</td>\n",
       "      <td>1547138020663</td>\n",
       "      <td>Repost</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>1085739328632995842</td>\n",
       "      <td>70320798</td>\n",
       "      <td>yono kojiro</td>\n",
       "      <td>yo_kojiro</td>\n",
       "      <td>371</td>\n",
       "      <td>Masih berpikir mau beli mobil selain Toyota Avanza.....???\\n\\nHaaaiii Kalimantan Barat... Mau service tanpa antri, yang lagi mager atau yg sambil bersihin rumah, kuras bak mandi, ngepel‚Ä¶ https://t.co/rFg6KCK2uc</td>\n",
       "      <td>West Borneo, Indonesia</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>ID</td>\n",
       "      <td>109.368618</td>\n",
       "      <td>-0.085911</td>\n",
       "      <td>in</td>\n",
       "      <td>2019-01-17T03:23:22+00:00</td>\n",
       "      <td>1547695402664</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1085780990646194177</td>\n",
       "      <td>911926560</td>\n",
       "      <td>Ana Nabila Olshop</td>\n",
       "      <td>oelphe_imoet</td>\n",
       "      <td>84</td>\n",
       "      <td>Apa anak-anak Bunda sering mengalami mimpi buruk?\\nAtau nggak mau pergi ke sekolah?\\nAtau pergi ke sekolah, tapi malah bolos? ü§î\\n.\\nBisa jadi, itu karena efek bullying yang ada di sekolahnya,‚Ä¶ https://t.co/xer8lH7Ht5</td>\n",
       "      <td>Jambangan, Indonesia</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>ID</td>\n",
       "      <td>112.712989</td>\n",
       "      <td>-7.326160</td>\n",
       "      <td>in</td>\n",
       "      <td>2019-01-17T06:08:55+00:00</td>\n",
       "      <td>1547705335662</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>412 rows √ó 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id             user_id            user_name  \\\n",
       "0    1080002955217129472           132400580        Alfonso Bueno   \n",
       "1    1080005505337249792          2209026173             aylnsnmz   \n",
       "2    1080005899618537473            35763405           Alex Amado   \n",
       "3    1080006864295874560  702750804894052353        Ananth Vikram   \n",
       "4    1080007355020890113            16356124           markcoccio   \n",
       "..                   ...                 ...                  ...   \n",
       "407  1080878759417430016            92357502         Afra Augesti   \n",
       "408  1083179888805380097  773709250182934532         jogjajourney   \n",
       "409  1083401499076640773          2726285827  Cinere Scooter Club   \n",
       "410  1085739328632995842            70320798          yono kojiro   \n",
       "411  1085780990646194177           911926560    Ana Nabila Olshop   \n",
       "\n",
       "    user_screen_name  user_followers_count  \\\n",
       "0         alfonso_b1                   134   \n",
       "1            mcrkz16                    74   \n",
       "2      AlexAmado2002                   547   \n",
       "3     AnanthVikram85                    67   \n",
       "4         markcoccio                  1339   \n",
       "..               ...                   ...   \n",
       "407      avraaugesty                   691   \n",
       "408    jogjajourney1                    33   \n",
       "409      VespaCinere                  1782   \n",
       "410        yo_kojiro                   371   \n",
       "411     oelphe_imoet                    84   \n",
       "\n",
       "                                                                                                                                                                                                                                  text  \\\n",
       "0                                                                                                                                            Feliz a√±o nuevo! Happy new year! #2019 @ Nuevo Laredo, Tamaulipas https://t.co/dVMk1Pl7Et   \n",
       "1                                                                                   Happy new year olur in≈üallahüôèüèºüòÖ #konuyubiliyorsunTanrƒ±m(amin) (@ Home Sweet Homeüè°üíö in Toronto, ON) https://t.co/ruozePd27j https://t.co/eM3rOeQHgF   \n",
       "2                                                                                                                                                Wishing everyone a Happy New 2019! @ At Cheonggyecheon Stream https://t.co/8imInI7vaN   \n",
       "3                      Wishing All a very Happy and Artistic New Year 2019 ahead!! üéÇüç´üôèüòá \\nIf around Bangalore South, kindly make over for a solo recital by my Baby Mahati Ananth @ National College Grounds,‚Ä¶ https://t.co/MzdwCGo4ri   \n",
       "4                         Happy 2019 Everyone.....man oh man the goals I have this year are HUGE and scary, but stand by for updates and check-ins along the way, it is going to be fun. @ Burbank, California https://t.co/g553Myz1Su   \n",
       "..                                                                                                                                                                                                                                 ...   \n",
       "407                 Malam ini ceritanya lagi mau nyoba ngedit video di laptop. Tetiba nemu 3 rekaman \"dibuang sayang\" ini.\\n.\\nEto-etonya, di video itu, kami lagi diwawancara penyiar Capital FM, salah satu‚Ä¶ https://t.co/5FvQJGT8iA   \n",
       "408         Mantai yuuk...\\n\\nüì∑ by laila_journey \\n\\nMau?\\nReservasi sekarang juga !\\nCall/text/WA/TG : +6287705576633\\n\\nMore information :\\naristiana_jogjajourney lopinda_journey @utamipipit nisrinanazih‚Ä¶ https://t.co/jT6HS63Yi7   \n",
       "409  #Repost from scooter_beang_kerok with https://t.co/bdeEaPnh03 \\n ... \\n\\nReposted from pesi_official -  Buat sedulur se vespa mari budayakan kembali 1 VESPA BERJUTA SODARA.. jadi...bagi sedulur yg mau‚Ä¶ https://t.co/U2ozy7mmrV   \n",
       "410                 Masih berpikir mau beli mobil selain Toyota Avanza.....???\\n\\nHaaaiii Kalimantan Barat... Mau service tanpa antri, yang lagi mager atau yg sambil bersihin rumah, kuras bak mandi, ngepel‚Ä¶ https://t.co/rFg6KCK2uc   \n",
       "411           Apa anak-anak Bunda sering mengalami mimpi buruk?\\nAtau nggak mau pergi ke sekolah?\\nAtau pergi ke sekolah, tapi malah bolos? ü§î\\n.\\nBisa jadi, itu karena efek bullying yang ada di sekolahnya,‚Ä¶ https://t.co/xer8lH7Ht5   \n",
       "\n",
       "                       place_name      place_country place_country_code  \\\n",
       "0        Nuevo Laredo, Tamaulipas             Mexico                 MX   \n",
       "1                Toronto, Ontario             Canada                 CA   \n",
       "2    Jongno-gu, Republic of Korea  Republic of Korea                 KR   \n",
       "3          Bengaluru South, India              India                 IN   \n",
       "4                     Burbank, CA      United States                 US   \n",
       "..                            ...                ...                ...   \n",
       "407             Dhaka, Bangladesh         Bangladesh                 BD   \n",
       "408          Danurejan, Indonesia          Indonesia                 ID   \n",
       "409               Lima, Indonesia          Indonesia                 ID   \n",
       "410        West Borneo, Indonesia          Indonesia                 ID   \n",
       "411          Jambangan, Indonesia          Indonesia                 ID   \n",
       "\n",
       "      longitude   latitude lang                       date      timestamp  \\\n",
       "0    -99.507100  27.485600   es  2019-01-01T07:29:04+00:00  1546327744665   \n",
       "1    -79.397235  43.759598   en  2019-01-01T07:39:12+00:00  1546328352661   \n",
       "2    126.978992  37.569142   en  2019-01-01T07:40:46+00:00  1546328446665   \n",
       "3     77.544640  12.939070   en  2019-01-01T07:44:36+00:00  1546328676662   \n",
       "4   -118.342900  34.169200   en  2019-01-01T07:46:33+00:00  1546328793660   \n",
       "..          ...        ...  ...                        ...            ...   \n",
       "407   90.433209  23.813322   in  2019-01-03T17:29:12+00:00  1546536552658   \n",
       "408  110.368000  -7.795850   in  2019-01-10T01:53:04+00:00  1547085184657   \n",
       "409  106.782747  -6.334487   in  2019-01-10T16:33:40+00:00  1547138020663   \n",
       "410  109.368618  -0.085911   in  2019-01-17T03:23:22+00:00  1547695402664   \n",
       "411  112.712989  -7.326160   in  2019-01-17T06:08:55+00:00  1547705335662   \n",
       "\n",
       "                  hashtag_0 hashtag_1 hashtag_2  \n",
       "0                       NaN       NaN       NaN  \n",
       "1    konuyubiliyorsunTanrƒ±m       NaN       NaN  \n",
       "2                       NaN       NaN       NaN  \n",
       "3                       NaN       NaN       NaN  \n",
       "4                       NaN       NaN       NaN  \n",
       "..                      ...       ...       ...  \n",
       "407                     NaN       NaN       NaN  \n",
       "408                     NaN       NaN       NaN  \n",
       "409                  Repost       NaN       NaN  \n",
       "410                     NaN       NaN       NaN  \n",
       "411                     NaN       NaN       NaN  \n",
       "\n",
       "[412 rows x 17 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res,dic=search_by_serveral_word_or(\"happy,yappy,Mau\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'yappy': ['happy',\n",
       "  'appt',\n",
       "  'üêΩhappy',\n",
       "  'ü•Çhappy',\n",
       "  '(happy',\n",
       "  'üéâhappy',\n",
       "  'rappa',\n",
       "  'yatay',\n",
       "  'happ≈∑',\n",
       "  'yayyy',\n",
       "  'yapƒ±≈ü',\n",
       "  'apps',\n",
       "  'yapƒ±',\n",
       "  'puppy',\n",
       "  'yapƒ±m',\n",
       "  'apply',\n",
       "  '2happy',\n",
       "  'happpy']}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code qui permet de sortir la liste country\n",
    "l=sorted(list(tweet[\"place_country\"].dropna().unique()))\n",
    "f=open('liste_country.txt','w')\n",
    "for ele in l:\n",
    "    f.write(ele+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>user_followers_count</th>\n",
       "      <th>text</th>\n",
       "      <th>place_name</th>\n",
       "      <th>place_country</th>\n",
       "      <th>place_country_code</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>lang</th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>hashtag_0</th>\n",
       "      <th>hashtag_1</th>\n",
       "      <th>hashtag_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1080004972647989249</td>\n",
       "      <td>99873362</td>\n",
       "      <td>JON RECRDS</td>\n",
       "      <td>juan_ON_</td>\n",
       "      <td>523</td>\n",
       "      <td>18-19\\n2018 fue el a√±o de la fuerza de atracci√≥n. De comprobar que lo que deseas te llega... que vengan muchos a√±os as√≠ o mejores!üôåüèΩ #gracias! \\n#photography #2018 \\n#2019 #happynewyear‚Ä¶ https://t.co/tdrsJQIhhe</td>\n",
       "      <td>Monterrey, Nuevo Le√≥n</td>\n",
       "      <td>M√©xico</td>\n",
       "      <td>MX</td>\n",
       "      <td>-100.300000</td>\n",
       "      <td>25.666700</td>\n",
       "      <td>es</td>\n",
       "      <td>2019-01-01T07:37:05+00:00</td>\n",
       "      <td>1546328225658</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1080007313090625536</td>\n",
       "      <td>799284810</td>\n",
       "      <td>Oscar Ornelas</td>\n",
       "      <td>oscarornelas512</td>\n",
       "      <td>37</td>\n",
       "      <td>#happynewyear @ The Skylark Lounge https://t.co/X6XKcUGfEz</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>United States</td>\n",
       "      <td>US</td>\n",
       "      <td>-97.704650</td>\n",
       "      <td>30.284490</td>\n",
       "      <td>en</td>\n",
       "      <td>2019-01-01T07:46:23+00:00</td>\n",
       "      <td>1546328783663</td>\n",
       "      <td>happynewyear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1080009531869020161</td>\n",
       "      <td>235531227</td>\n",
       "      <td>Khyati Ratan</td>\n",
       "      <td>KhyatiRatan</td>\n",
       "      <td>109</td>\n",
       "      <td>Celebrate endings ‚Äì for they precede new beginnings - Jonathan Lockwood Huie, author\\n#newyearseve #nye #newyear #christmas #party #newyears #happynewyear #dj #love #music #holiday‚Ä¶ https://t.co/9iI7vp5p2p</td>\n",
       "      <td>Mumbai, India</td>\n",
       "      <td>India</td>\n",
       "      <td>IN</td>\n",
       "      <td>72.876400</td>\n",
       "      <td>19.062100</td>\n",
       "      <td>en</td>\n",
       "      <td>2019-01-01T07:55:12+00:00</td>\n",
       "      <td>1546329312661</td>\n",
       "      <td>newyearseve</td>\n",
       "      <td>nye</td>\n",
       "      <td>newyear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1080010236512034817</td>\n",
       "      <td>233411963</td>\n",
       "      <td>Anthony BOY Flores</td>\n",
       "      <td>boyrack87</td>\n",
       "      <td>814</td>\n",
       "      <td>#happynewyear üéá everyone. #2019 and beyond is about growth. I've kept it real for long enough. There's not one dam person on this earth that can make me feel less confident other then‚Ä¶ https://t.co/tzwwzPGIol</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>United States</td>\n",
       "      <td>US</td>\n",
       "      <td>-118.243000</td>\n",
       "      <td>34.052200</td>\n",
       "      <td>en</td>\n",
       "      <td>2019-01-01T07:58:00+00:00</td>\n",
       "      <td>1546329480661</td>\n",
       "      <td>happynewyear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1080010769171804160</td>\n",
       "      <td>20509373</td>\n",
       "      <td>The Real JWT</td>\n",
       "      <td>RealJWT</td>\n",
       "      <td>121</td>\n",
       "      <td>Champagne Wishes &amp;amp; Caviar Dreams! ü•Ç Happy 2019! #champagne #newyear #2019 #cheers #california #losangeles #family #newbeginnings #happy #celebrate #crystal #beautiful #countdown #holiday‚Ä¶ https://t.co/MvSlLpeeOq</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>United States</td>\n",
       "      <td>US</td>\n",
       "      <td>-118.243000</td>\n",
       "      <td>34.052200</td>\n",
       "      <td>en</td>\n",
       "      <td>2019-01-01T08:00:07+00:00</td>\n",
       "      <td>1546329607657</td>\n",
       "      <td>champagne</td>\n",
       "      <td>newyear</td>\n",
       "      <td>cheers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18572</th>\n",
       "      <td>1085701441489113088</td>\n",
       "      <td>2573230169</td>\n",
       "      <td>Jean Braga</td>\n",
       "      <td>Everson_braga31</td>\n",
       "      <td>16</td>\n",
       "      <td>Felicidade sem motivo, melhor imposs√≠vel!\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\nüòÑ #happy #toptags #happydays #happyday #smile #fun #instahappy #goodmood #sohappy #happier #excited #feelgood #smiling #funtimes‚Ä¶ https://t.co/WxkbWUAZX7</td>\n",
       "      <td>Fortaleza, Brasil</td>\n",
       "      <td>Brasil</td>\n",
       "      <td>BR</td>\n",
       "      <td>-38.540554</td>\n",
       "      <td>-3.714208</td>\n",
       "      <td>pt</td>\n",
       "      <td>2019-01-17T00:52:49+00:00</td>\n",
       "      <td>1547686369665</td>\n",
       "      <td>happy</td>\n",
       "      <td>toptags</td>\n",
       "      <td>happydays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18709</th>\n",
       "      <td>1085718201915170816</td>\n",
       "      <td>46345790</td>\n",
       "      <td>WedKes</td>\n",
       "      <td>WedKes</td>\n",
       "      <td>35</td>\n",
       "      <td>NY Party 2019 #newyear #party #data #architecture #team #gsb #dinner #happy #perfect @ ‡∏Ñ‡∏£‡∏±‡∏ß‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û ‡∏û‡∏ç‡∏≤‡πÑ‡∏ó https://t.co/I3RRgyTOef</td>\n",
       "      <td>Thanon Phaya Thai, Thailand</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>TH</td>\n",
       "      <td>100.532832</td>\n",
       "      <td>13.755291</td>\n",
       "      <td>th</td>\n",
       "      <td>2019-01-17T01:59:25+00:00</td>\n",
       "      <td>1547690365662</td>\n",
       "      <td>newyear</td>\n",
       "      <td>party</td>\n",
       "      <td>data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18772</th>\n",
       "      <td>1085728998062055424</td>\n",
       "      <td>183895938</td>\n",
       "      <td>Harsh Gupta</td>\n",
       "      <td>fairngood</td>\n",
       "      <td>66</td>\n",
       "      <td>Good Morning Everyone \\n\\n#behappy #stayhappy #keepsmiling #keepgrowing @ Madhav Niketan https://t.co/Fr75xNwDZP</td>\n",
       "      <td>Ludhiana, India</td>\n",
       "      <td>India</td>\n",
       "      <td>IN</td>\n",
       "      <td>75.777905</td>\n",
       "      <td>30.870134</td>\n",
       "      <td>en</td>\n",
       "      <td>2019-01-17T02:42:19+00:00</td>\n",
       "      <td>1547692939664</td>\n",
       "      <td>behappy</td>\n",
       "      <td>stayhappy</td>\n",
       "      <td>keepsmiling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18807</th>\n",
       "      <td>1085733582428037120</td>\n",
       "      <td>65730236</td>\n",
       "      <td>Bullet Mckenzie</td>\n",
       "      <td>BulletMcK</td>\n",
       "      <td>592</td>\n",
       "      <td>My favorite part of Rockn Roll Weekend was being able to cook and feed the fam at @eatbeerfish.  Thank you all for coming down.  So happy to hang with my rockn roll sister and‚Ä¶ https://t.co/WFT9qIDwa9</td>\n",
       "      <td>San Diego, CA</td>\n",
       "      <td>United States</td>\n",
       "      <td>US</td>\n",
       "      <td>-117.130630</td>\n",
       "      <td>32.762640</td>\n",
       "      <td>en</td>\n",
       "      <td>2019-01-17T03:00:32+00:00</td>\n",
       "      <td>1547694032662</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19183</th>\n",
       "      <td>1085787080754528257</td>\n",
       "      <td>2227460192</td>\n",
       "      <td>–ê–ª–µ–∫—Å–∞–Ω–¥—Ä –¢–æ—á–∫–∞</td>\n",
       "      <td>TochkaAleksandr</td>\n",
       "      <td>391</td>\n",
       "      <td>–ö–Ω–∏–∂–Ω–∞—è –µ–ª–∫–∞ –Ω–∞ —Ñ–µ—Å—Ç–∏–≤–∞–ª–µ #–ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–µ–≤—Ä–æ–∂–¥–µ—Å—Ç–≤–æ #–º–æ—Å–∫–æ–≤—Å–∫–∏–µ—Å–µ–∑–æ–Ω—ã #–±—É–ª–≥–∞–∫–æ–≤ #world #beauty #bestfoto #life #–º–∏—Ä #–ø–æ–∑–∏—Ç–∏–≤ #positive #instagood #photooftheday #happy #beautiful‚Ä¶ https://t.co/GtBhCHdyoq</td>\n",
       "      <td>–ú–æ—Å–∫–≤–∞, –†–æ—Å—Å–∏—è</td>\n",
       "      <td>–†–æ—Å—Å–∏—è</td>\n",
       "      <td>RU</td>\n",
       "      <td>37.619057</td>\n",
       "      <td>55.761604</td>\n",
       "      <td>uk</td>\n",
       "      <td>2019-01-17T06:33:07+00:00</td>\n",
       "      <td>1547706787657</td>\n",
       "      <td>–ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–µ–≤—Ä–æ–∂–¥–µ—Å—Ç–≤–æ</td>\n",
       "      <td>–º–æ—Å–∫–æ–≤—Å–∫–∏–µ—Å–µ–∑–æ–Ω—ã</td>\n",
       "      <td>–±—É–ª–≥–∞–∫–æ–≤</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>271 rows √ó 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id     user_id           user_name user_screen_name  \\\n",
       "16     1080004972647989249    99873362          JON RECRDS         juan_ON_   \n",
       "38     1080007313090625536   799284810       Oscar Ornelas  oscarornelas512   \n",
       "58     1080009531869020161   235531227        Khyati Ratan      KhyatiRatan   \n",
       "67     1080010236512034817   233411963  Anthony BOY Flores        boyrack87   \n",
       "72     1080010769171804160    20509373        The Real JWT          RealJWT   \n",
       "...                    ...         ...                 ...              ...   \n",
       "18572  1085701441489113088  2573230169          Jean Braga  Everson_braga31   \n",
       "18709  1085718201915170816    46345790              WedKes           WedKes   \n",
       "18772  1085728998062055424   183895938         Harsh Gupta        fairngood   \n",
       "18807  1085733582428037120    65730236     Bullet Mckenzie        BulletMcK   \n",
       "19183  1085787080754528257  2227460192     –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –¢–æ—á–∫–∞  TochkaAleksandr   \n",
       "\n",
       "       user_followers_count  \\\n",
       "16                      523   \n",
       "38                       37   \n",
       "58                      109   \n",
       "67                      814   \n",
       "72                      121   \n",
       "...                     ...   \n",
       "18572                    16   \n",
       "18709                    35   \n",
       "18772                    66   \n",
       "18807                   592   \n",
       "19183                   391   \n",
       "\n",
       "                                                                                                                                                                                                                            text  \\\n",
       "16            18-19\\n2018 fue el a√±o de la fuerza de atracci√≥n. De comprobar que lo que deseas te llega... que vengan muchos a√±os as√≠ o mejores!üôåüèΩ #gracias! \\n#photography #2018 \\n#2019 #happynewyear‚Ä¶ https://t.co/tdrsJQIhhe   \n",
       "38                                                                                                                                                                    #happynewyear @ The Skylark Lounge https://t.co/X6XKcUGfEz   \n",
       "58                Celebrate endings ‚Äì for they precede new beginnings - Jonathan Lockwood Huie, author\\n#newyearseve #nye #newyear #christmas #party #newyears #happynewyear #dj #love #music #holiday‚Ä¶ https://t.co/9iI7vp5p2p   \n",
       "67              #happynewyear üéá everyone. #2019 and beyond is about growth. I've kept it real for long enough. There's not one dam person on this earth that can make me feel less confident other then‚Ä¶ https://t.co/tzwwzPGIol   \n",
       "72       Champagne Wishes &amp; Caviar Dreams! ü•Ç Happy 2019! #champagne #newyear #2019 #cheers #california #losangeles #family #newbeginnings #happy #celebrate #crystal #beautiful #countdown #holiday‚Ä¶ https://t.co/MvSlLpeeOq   \n",
       "...                                                                                                                                                                                                                          ...   \n",
       "18572  Felicidade sem motivo, melhor imposs√≠vel!\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\nüòÑ #happy #toptags #happydays #happyday #smile #fun #instahappy #goodmood #sohappy #happier #excited #feelgood #smiling #funtimes‚Ä¶ https://t.co/WxkbWUAZX7   \n",
       "18709                                                                                           NY Party 2019 #newyear #party #data #architecture #team #gsb #dinner #happy #perfect @ ‡∏Ñ‡∏£‡∏±‡∏ß‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û ‡∏û‡∏ç‡∏≤‡πÑ‡∏ó https://t.co/I3RRgyTOef   \n",
       "18772                                                                                                           Good Morning Everyone \\n\\n#behappy #stayhappy #keepsmiling #keepgrowing @ Madhav Niketan https://t.co/Fr75xNwDZP   \n",
       "18807                   My favorite part of Rockn Roll Weekend was being able to cook and feed the fam at @eatbeerfish.  Thank you all for coming down.  So happy to hang with my rockn roll sister and‚Ä¶ https://t.co/WFT9qIDwa9   \n",
       "19183                   –ö–Ω–∏–∂–Ω–∞—è –µ–ª–∫–∞ –Ω–∞ —Ñ–µ—Å—Ç–∏–≤–∞–ª–µ #–ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–µ–≤—Ä–æ–∂–¥–µ—Å—Ç–≤–æ #–º–æ—Å–∫–æ–≤—Å–∫–∏–µ—Å–µ–∑–æ–Ω—ã #–±—É–ª–≥–∞–∫–æ–≤ #world #beauty #bestfoto #life #–º–∏—Ä #–ø–æ–∑–∏—Ç–∏–≤ #positive #instagood #photooftheday #happy #beautiful‚Ä¶ https://t.co/GtBhCHdyoq   \n",
       "\n",
       "                        place_name  place_country place_country_code  \\\n",
       "16           Monterrey, Nuevo Le√≥n         M√©xico                 MX   \n",
       "38                      Austin, TX  United States                 US   \n",
       "58                   Mumbai, India          India                 IN   \n",
       "67                 Los Angeles, CA  United States                 US   \n",
       "72                 Los Angeles, CA  United States                 US   \n",
       "...                            ...            ...                ...   \n",
       "18572            Fortaleza, Brasil         Brasil                 BR   \n",
       "18709  Thanon Phaya Thai, Thailand       Thailand                 TH   \n",
       "18772              Ludhiana, India          India                 IN   \n",
       "18807                San Diego, CA  United States                 US   \n",
       "19183               –ú–æ—Å–∫–≤–∞, –†–æ—Å—Å–∏—è         –†–æ—Å—Å–∏—è                 RU   \n",
       "\n",
       "        longitude   latitude lang                       date      timestamp  \\\n",
       "16    -100.300000  25.666700   es  2019-01-01T07:37:05+00:00  1546328225658   \n",
       "38     -97.704650  30.284490   en  2019-01-01T07:46:23+00:00  1546328783663   \n",
       "58      72.876400  19.062100   en  2019-01-01T07:55:12+00:00  1546329312661   \n",
       "67    -118.243000  34.052200   en  2019-01-01T07:58:00+00:00  1546329480661   \n",
       "72    -118.243000  34.052200   en  2019-01-01T08:00:07+00:00  1546329607657   \n",
       "...           ...        ...  ...                        ...            ...   \n",
       "18572  -38.540554  -3.714208   pt  2019-01-17T00:52:49+00:00  1547686369665   \n",
       "18709  100.532832  13.755291   th  2019-01-17T01:59:25+00:00  1547690365662   \n",
       "18772   75.777905  30.870134   en  2019-01-17T02:42:19+00:00  1547692939664   \n",
       "18807 -117.130630  32.762640   en  2019-01-17T03:00:32+00:00  1547694032662   \n",
       "19183   37.619057  55.761604   uk  2019-01-17T06:33:07+00:00  1547706787657   \n",
       "\n",
       "                   hashtag_0         hashtag_1    hashtag_2  \n",
       "16                       NaN               NaN          NaN  \n",
       "38              happynewyear               NaN          NaN  \n",
       "58               newyearseve               nye      newyear  \n",
       "67              happynewyear               NaN          NaN  \n",
       "72                 champagne           newyear       cheers  \n",
       "...                      ...               ...          ...  \n",
       "18572                  happy           toptags    happydays  \n",
       "18709                newyear             party         data  \n",
       "18772                behappy         stayhappy  keepsmiling  \n",
       "18807                    NaN               NaN          NaN  \n",
       "19183  –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–µ–≤—Ä–æ–∂–¥–µ—Å—Ç–≤–æ  –º–æ—Å–∫–æ–≤—Å–∫–∏–µ—Å–µ–∑–æ–Ω—ã     –±—É–ª–≥–∞–∫–æ–≤  \n",
       "\n",
       "[271 rows x 17 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_by_word(\"happy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brouillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs(x):\n",
    "    if dist_levenstein.DistanceDeLevenshtein(x.lower(),\"yahpy\".lower())<3:\n",
    "        print(x,dist_levenstein.DistanceDeLevenshtein(x.lower(),\"yappy\".lower()))\n",
    "datframe_word[\"word\"].apply(obs)\n",
    "tmp=tweet[tweet[\"text\"].str.contains(\"(?i)\\b\"+word+\"\\b\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bon format pour un mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requetes mot:\n",
    "res,l=search_by_word_only(tweet,\"Mau\")\n",
    "res.groupby(\"place_country\").count()\n",
    "#Avoir le plus grand nombre de tweet par  pays :\n",
    "res,l=search_by_word_only(tweet,\"Mau\")\n",
    "res.groupby(\"hashtag_0\").count().sort_values(\"id\",ascending=0)\n",
    "#Compte de tweet par mot\n",
    "words=\"Mau,Salat\"\n",
    "count_tweet_per_word=pd.DataFrame()\n",
    "res,l=search_by_serveral_word_or(words)\n",
    "for word in words.split(\",\"):\n",
    "    count_tweet_per_word=count_tweet_per_word.append({\"word\":word,\"count_tweet\":len(res[res[\"word\"]==word])},ignore_index=True)\n",
    "    #count_tweet_per_word.append({\"word\":word,\"count_tweet\":len(res[res[\"word\"]]==word)},ignore_index=True)\n",
    "\n",
    "#Avoir le top 10 hashtag:\n",
    "union_hashtag=get_union_hastag(tweet).groupby('hashtag').count().sort_values(\"index\",ascending=0).iloc[0:10]\n",
    "union_hashtag"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
